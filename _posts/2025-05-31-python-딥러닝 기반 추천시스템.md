---
layout: post
title:  "딥러닝 기반 추천시스템" 
subtitle:   "딥러닝 기반 추천시스템"
categories: DL
tags: RS
comments: true
---

## 딥러닝 기반 추천시스템 대한 정보를 남긴 포스팅 입니다.

[이전 포스팅(추천시스템 기초)](https://bluemumin.github.io/python/2025/05/19/python-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B8%B0%EC%B4%88/)

<br/>

### 1. 기존 모델의 문제점

추천시스템에서 기존에 쓰이던 Matrix Factorization은 

유저 - 아이템 상호 작용을 내적의 형태로 표현

-> 상호작용 데이터의 복잡한 구조는 알아내기가 어려움 +

신규 유저가 나타나면 이를 저차원 공간에 표현하기 어려움

<br/>

### 2. 딥러닝 기반 추천 시스템

- 비 선형 변환 가능
  
- 더 나은 representation learning : hand-craft feature design 줄이고, 다차원 정보 처리 가능
  
- 시퀀스 모델링 : RNN, CNN 통해 성능 향상

- 유연성 : 딥러닝의 학습 환경 활용 가능
  
<br/>

#### 2-1. [Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031)

GMF + MLP

유저 - 아이템 상호작용 표현을 위해, Matrix Factorization의 선형성 + 

Multilayer Perceptron의 비 선형성을 결합한 Neural Matrix Factorization

선형 공간에 기반한 기존 모델 한계를 Deep Neural Network를 도입하여 해결.

<img data-action="zoom" src='{{ "/assets/img/recommend_system/Neural_collaborative_filtering.png" | relative_url }}' alt='absolute'> 

<br/>

##### Generalized Matrix Factorization

Matrix Factorization의 확장버전. 내적 연산 외에도 다양한 연산 가능. 

아래 방식을 통해, 선형 구조를 가지게 됨.

<br/>

a. input layer : input으로 유저와 아이템의 one-hot vector 넣음.

b. embedding layer : input 단계의 sparse 벡터를 dense 벡터로 매핑.

c. element-wise product : k차원의 feature에 대응하는 값끼리 가중합.

원하는 특징에 따라 가중치 조정 가능하게 일반화.

d. Passing Fully Connected Layer : 가중치를 학습하여, 유저 - 아이템의 행렬값 예측

<br/>

##### Multilayer Perceptron 

concatenate 되어, 비선형의 활성화함수를 거치게 되기에, 비선형 구조를 가지게 됨.

단순 상호작용만 되던 기존 구조에서 MLP 추가 시, 표현력이 높아지고

조건부 / 복잡한 상호작용 학습이 가능해짐.

<br/>

기본 구성 : (선형 변환 + 비선형 활성화 함수 구성)

input layer + Hidden layers (1개 이상) + 

activation function (ReLU, tanh 등) + Output layer(sigmoid, softmax 등)

<br/>

#### 2-2. [RNN-Based Model](https://arxiv.org/pdf/1511.06939)

기존 Session-based의 어려움 :

유저가 로그인하지 않은 환경에서는 latent vector 사용이 어려움

session이나 cookie 매커니즘이 있지만 데이터가 적음

-> 유저의 순서정보 활용 with RNN

(어떤 아이템을 경험했는가? + 가장 최근에 클릭한 아이템)

<br/>

##### Gated Recurrent Units(GRU)

a. reset Gate : 과거의 정보를 적절하게 리셋

b. Update Gate : LSTM의 forget gate + input gate, (과거와 현재 정보 최신화 비율 결정)

c. Candidate : 현 시점의 정보 후보군 계산, 과거 은닉층 정보를 그대로 이용하는 것이 아닌,

리셋 게이트의 결과를 곱해서 이용 (tanh)

d. Hidden State 계산 : update gate, candidate 결과 결합 -> 현 시점의 은닉층 계산

<br/>

##### 모델 구조

1.input layer : 세션의 현재 상태를 벡터 형태로 삽입. 

(1-of-N encoding vector) : 어떤 아이템이 클릭 되었는지 나타내는 one-hot encoding 벡터.

사용자가 어떤 세션에서 여러 개 아이템 클릭 했다면,

이러한 이벤트를 하나의 벡터로 압축해서 삽입 . (먼저 일어난 이벤트일수록 가중치 낮게)

2.Embedding Layer : long term 문제 극복을 위한 정규화

3.GRU Layer : 아이템에 대한 선호도 예측 학습

4.Output Layer : 해당 세선의 다음 이벤트 아이템 출력

<br/>

##### Session-parallel Mini-batch

input을 세션 병렬 방식의 mini-batch로 넣음.

다음 세션 정보가 들어올 때, hidden state 초기화.

<br/>

##### Sampling the Output

모든 아이템에 대한 loss 계산을 하지 않기 위해 Output을 샘플링 +

negative sampling : 사용자가 클릭하지 않은 것은 negative로 간주하며 학습.

(마치 binary classification.)

Popularity-based Sampling : 자주 클릭 된 아이템 중에서 negative 후보 뽑음.

이런 아이템은 모델이 잘 구분해야될 헷갈리는 예시로 사용 가능.

-> 학습 효율 향상 + 일반화 성능 향상됨.

<br/>

##### Pairwise Ranking

positive & negative 쌍으로 비교.

- BPR Loss : positive 아이템 점수가 negative보다 크도록 만듦

ranking에만 집중. (점수 절댓값에는 관심 x)

직관적이고 효과적인 랭킹 기반 손실 함수.

- TOP1 Loss : positive가 더 높은 점수를 갖게 하면서,

negative의 점수가 작게 되도록 억제함 (안정적 학습 유도 / like 정규화)

<br/>

#### 2-3. [NARM](https://arxiv.org/pdf/1711.04725)

"Neural Attentive Session-based Recommendation"

세션 기반 추천 + 순서 학습 + "사용자의 목표 파악"

불필요한 정보는 무시하고 의도 파악을 더 정확하게 해줌.

Attention으로 중간이나 초반 중요 클릭도 반영하므로 세션이 길어도 더 안정적 추천 가능.

MLP 등으로 계산하는 것이 아닌, bi-linear 구조인 간단한 수식을 사용해 

선형 연산만 하므로, 효율적이고 빠르며 병렬화 가능함.

<br/>

<img data-action="zoom" src='{{ "/assets/img/recommend_system/narm.png" | relative_url }}' alt='absolute'> 

1.input layer : 세션에 속한 아이템 시퀀스

2.Embedding Layer : 각 아이템을 고정 차원의 dense vector 변환

3.GRU encoder Layer : 순서를 따라 시퀀스를 인코딩 (RNN 구조 사용)

(global encoder : 사용자의 일반적인 탐색 패턴 파악 / GRU의 마지막 hidden state)

4.Attention network Layer : 세션 내 아이템들 중 어떤 것이 현재 목표와 더 관련 깊은지 가중치 계산

(local encoder : 사용자의 현재 의도 파악 / GRU의 모든 hidden state에 attention을 적용) 

5.Session representation : 순차 정보(GRU) + 목표 정보(attention) concat

6.Output Layer : softmax or ranking loss로 다음 아이템 예측

<br/>

#### 2-4. AE-based Model

AutoEncoder or ([VAE](https://arxiv.org/pdf/1312.6114)) + 추천시스템  

Encoder의 차원 축소 -> Latent Sapce(저차원의 잠재 공간) 찾기 가능

Decoder가 Input Data를 복원함 -> Latent Model과 유사함

- 비선형 활성 함수 활용, 사용자 간의 관계를 모델링하기에 성능이 좋음

- 단순 Matrix Factorization에 비해, sparse matrix(0이 많이 포함된 것) 에서 좋은 성능을 보임.

- 활용 범위 : 평점, 순위 추천 / 클릭, 시청 여부 등에 모두 활용 가능

<br/>

축소된 차원인 Latent space를 활용한 추천 가능

-> 전처리 된 컨텐츠 matrix를 축소하여, 축소된 차원에서 Cosine similarity로

거리를 측정하여 컨텐츠 간의 유사도를 구하고 이를 기준으로 추천 가능.

<br/>

또한, Matrix completion을 활용해서도 가능함.

사용자의 컨텐츠 시청 정보를 복원시켜, 복원된 matrix 바탕으로

사용자마다 컨텐츠 시청 확률 출력 -> 높은 순서대로 컨텐츠 추천 가능.

<br/>

Denoising AutoEncoder : noise가 추가된 input data를 noise가 없는 input data로 복원하는 방식의 모델.

Variational AutoEncoder : 축소된 차원으로 Z를 샘플링 하여, Z를 정규분포에 가깝게 만드는 방식의 모델.

(latent space가 정규분포에 가깝게 되어, 다루기 쉽고 sparse한 부분이 줄어들어, 더 유용한 space 획득 가능.)

해당 복원된 matrix를 활용한 추천시스템 사용 가능.

<br/>

[AutoRec](https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf)


<br/>

[딥러닝 기반 추천 시스템 포스팅 출처 1](https://velog.io/@tobigs-recsys/DL-based-Recommender-Systems-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B8%B0%EB%B0%98-%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C#neural-collaborative-filtering)

